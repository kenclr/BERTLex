# Introduction
----------
BERTological Lexicography for Prepositions
# Abstract
----------
Contextualized word embedding (CWE) models such as BERT (Devlin et al. (2019)) have been used in many NLP tasks. Gessler and
Schneider (2021) (G&S) focus on the use of BERT in disambiguating
rare senses of nouns, verbs, and prepositions. In examining preposition disambiguation, their results are comparable to earlier efforts and
achieving better results than problems described in Litkowski (2013).
Earlier efforts achieved results using traditional NLP methods, characterizing syntactic and semantic behaviors. BERTology provides a new
resource and additional perspective that might assist in usual lexicographic procedures. We begin exactly at the place where G&S ended,
using its methods, adding the further property of keeping a link to the
instances in the Pattern Dictionary of English Prepositions (PDEP,
Litkowski (2014)), enabling further lexicographic analysis
# References
----------
- [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171â€“4186,
Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/
N19-1423
