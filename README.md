# Introduction
BERTological Lexicography for Prepositions
# Abstract
Contextualized word embedding (CWE) models such as BERT (De-
vlin et al. (2019)) have been used in many NLP tasks. Gessler and
Schneider (2021) (G&S) focus on the use of BERT in disambiguating
rare senses of nouns, verbs, and prepositions. In examining preposi-
tion disambiguation, their results are comparable to earlier efforts and
achieving better results than problems described in Litkowski (2013).
Earlier efforts achieved results using traditional NLP methods, charac-
terizing syntactic and semantic behaviors. BERTology provides a new
resource and additional perspective that might assist in usual lexico-
graphic procedures. We begin exactly at the place where G&S ended,
using its methods, adding the further property of keeping a link to the
instances in the Pattern Dictionary of English Prepositions (PDEP,
Litkowski (2014)), enabling further lexicographic analysis
